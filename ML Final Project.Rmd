---
title: "Machine Learning Final Project"
author: "Yilin Liu, Ding He"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  chunk_output_type: console
---

# libraries
```{r}
library(readr)
library(corrplot)
library(caret)
library(ggplot2)
```

# original data
```{r}
#setwd("C:/Users/hedin/OneDrive/Documents/HU/ANLY530/final project/group project/heart_disease_ML")
data <- read.csv("heart.csv")
head(data)
str(data)
```


# Data Cleaning and transformation

## factor the variables
```{r}
data$target= as.factor(data$target)
data$sex= as.factor(data$sex)
data$cp= as.factor(data$cp)
data$fbs= as.factor(data$fbs)
data$restecg= as.factor(data$restecg)
data$exang= as.factor(data$exang)
data$slope= as.factor(data$slope)
data$ca= as.factor(data$ca)
data$thal= as.factor(data$thal)
```

```{r}
str(data)
```


## Imputing Missing/NA
```{r}
sum(is.na(data))
```


## Imputing Outliers - ignore

## scale data /preprocess data - ignore this time

# Exploraring data
## correlation analysis
```{r}

```

## stats summary
```{r}
summary(data)
```

## Linear plot
```{r}

```

# Modeling

## Decision Tree
### Create a list of 80% of the rows in the Original dataset we can use for training
```{r}
index = createDataPartition(data[,1],p=0.80,list = FALSE)
dim(index)
```
### Use 80% of the data to training the model.
```{r}
training = data[index,]
dim(training)
```

```{r}
valid = data[-index,]
dim(valid)
```
### create test harnesses
```{r}
control <- trainControl(method="cv", number=10)
metric <- "Accuracy"
```

### Build a Decision Tree Model using rpart
```{r}
set.seed(7)
fit.rpart <- train(target~., data = training, method="rpart", metric=metric, trControl=control)
```

### Summarize the Results Briefly
```{r}
fit.rpart
```

```{r}
summary(fit.rpart$finalModel)
```

```{r}
suppressMessages(library(rattle))
fancyRpartPlot(fit.rpart$finalModel)
```
### Create Prediciton using Trained Decision Tree'
```{r}
data.pred = predict(fit.rpart, newdata = valid)
table(data.pred, valid$target)
```
### error
```{r}
error.rate = round(mean(data.pred != valid$target,2))
error.rate
```
### Confusion Matrix
```{r}
cm = confusionMatrix(as.factor(data.pred), reference = as.factor(valid$target), mode = "prec_recall")
print(cm)
```

## Random Forest
```{r}
library(AppliedPredictiveModeling)
```

```{r}
transparentTheme(trans = .13)
featurePlot(x = data[, 1:13], 
            y = data$target, 
            plot = "pairs",
            ## Add a key at the top
            auto.key = list(columns = 2))
```

```{r}
set.seed(7)
fit.rf <- train(target~., data = training, method="rf", metric=metric, trControl=control)
```

```{r}
fit.rf
```

```{r}
summary(fit.rf$finalModel)
```

```{r}
plot(fit.rf)
```


```{r}
vi = varImp(fit.rf, scale = FALSE)
plot(vi, top = ncol(training)-1)
```

### create prediciton using Trained random forest
```{r}
data.pred = predict(fit.rf, newdata = valid)
table(data.pred, valid$target)
```

### check error
```{r}
error.rate = round(mean(data.pred != valid$target,2))
error.rate
```

### Confusion Matrix
```{r}
cm = confusionMatrix(as.factor(data.pred), reference = as.factor(valid$target), mode = "prec_recall")
print(cm)
```
## Random Forest Model using extra trees
```{r}
set.seed(7)
fit.extraTrees <- train(target~., data = training, method="rf", metric=metric, trControl=control)
```

```{r}
fit.extraTrees
```

```{r}
summary(fit.extraTrees$finalModel)
```

```{r}
plot(fit.extraTrees)
```

```{r}
data.pred = predict(fit.extraTrees, newdata = valid)
table(data.pred, valid$target)
```

```{r}
error.rate = round(mean(data.pred != valid$target,2))
error.rate
```


```{r}
cm = confusionMatrix(as.factor(data.pred), reference = as.factor(valid$target), mode = "prec_recall")
print(cm)
```


```{r}
results = resamples(list(rf=fit.rf, extraTrees = fit.extraTrees))
summary(results)
```

### visualize comparison
```{r}
dotplot(results)
```


## Logistic Regression
```{r}

```



## K-Means
```{r}
library(dplyr)
data2 <- data %>% 
  mutate(sex = if_else(sex == 1, "MALE", "FEMALE"),
         fbs = if_else(fbs == 1, ">120", "<=120"),
         exang = if_else(exang == 1, "YES" ,"NO"),
         cp = if_else(cp == 1, "ATYPICAL ANGINA",
                      if_else(cp == 2, "NON-ANGINAL PAIN", "ASYMPTOMATIC")),
         restecg = if_else(restecg == 0, "NORMAL",
                           if_else(restecg == 1, "ABNORMALITY", "PROBABLE OR DEFINITE")),
         slope = as.factor(slope),
         ca = as.factor(ca),
         thal = as.factor(thal),
         target = if_else(target == 1, "YES", "NO")
         ) %>% 
  mutate_if(is.character, as.factor) %>% 
  dplyr::select(target, sex, fbs, exang, cp, restecg, slope, ca, thal, everything())
```

```{r}
library(funModeling)
profiling_num(data2)


uns_df <- scale(data2[,10:14])

head(as_tibble(uns_df))

#distance <- get_dist(uns_df)
#head(distance)
#fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))

k2 <- kmeans(uns_df, 
             center = 2,
             nstart = 25  )

str(k2)
k2
k2$centers
table(k2$cluster) #Give a count of data points in each cluster

library(factoextra)
fviz_cluster(k2, data = uns_df)
```


```{r}
k3 <- kmeans(uns_df, centers = 3, nstart = 25)
k4 <- kmeans(uns_df, centers = 4, nstart = 25)
k5 <- kmeans(uns_df, centers = 5, nstart = 25)

p1 <- fviz_cluster(k2, geom = "point", data = uns_df)+
  ggtitle("k = 2")
p2 <- fviz_cluster(k3, geom = "point", data = uns_df)+
  ggtitle("k = 3")
p3 <- fviz_cluster(k4, geom = "point", data = uns_df)+
  ggtitle("k = 4")
p4 <- fviz_cluster(k5, geom = "point", data = uns_df)+
  ggtitle("k = 5")

p2
p3
p4

library(gridExtra)
grid.arrange(p1,p2,p3,p4, nrow = 2)
```


###Optimum Cluster Number
```{r}
# Elbow Method
set.seed(123)
fviz_nbclust(uns_df, kmeans, method = "wss")

# Average Silouette Method
fviz_nbclust(uns_df, kmeans, method = "silhouette")


# Gap Statistics
set.seed(123)

library(cluster)
gap_stat <- clusGap(uns_df, FUN = kmeans, nstart = 25, K.max = 10, B = 50)

print(gap_stat, method = "firstmax")

fviz_gap_stat


set.seed(123)

final <- kmeans(uns_df, 2, nstart = 25)
final
```


###Descriptive Statistics for Clusters
```{r}
df<-data2[,10:14] %>% 
  mutate(Cluster = final$cluster) %>% 
  group_by(Cluster) %>% 
  summarise_all("mean")
df
```


#Best K value
```{r}
wssplot <- function(df, nc=15, seed=1234){
  wss <- (nrow(data2)-1)*sum(apply(data2,2,var))
  for (i in 2:nc){
    set.seed(seed)
    wss[i] <- sum(kmeans(data2, centers=i)$withinss)}
  plot(1:nc, wss, type="b", xlab="Number of Clusters",
       ylab="Within groups sum of squares")}

library(base)
data3 <- data.frame(lapply(data2, function(x) as.numeric(as.character(x))))
df<-scale(data3[-1])
head(df)
wssplot(df)
```


#K-means analysis 
#Start the k-Means analysis using the variable “nc” for the number of clusters
```{r}
library(NbClust)
set.seed(1234)
nc <- NbClust(df, min.nc=2, max.nc = 15, method = "kmeans")
```


#K-means analysis
```{r}
barplot(table(nc$Best.n[1,]), xlab = "Number of Clusters", ylab = "Number of Criteria", main = "Number of Clusters Chosen by 26 Criteria")
```





