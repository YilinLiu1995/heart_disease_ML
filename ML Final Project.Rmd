---
title: "Machine Learning Final Project"
author: "Yilin Liu, Ding He"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  chunk_output_type: console
---

# libraries
```{r}
library(readr)
library(corrplot)
library(caret)
library(ggplot2)
```

# original data
```{r}
data <- read.csv("heart.csv")
head(data)
str(data)
```


# Data Cleaning and transformation

## factor the variables
```{r}
data$target= as.factor(data$target)
data$sex= as.factor(data$sex)
data$cp= as.factor(data$cp)
data$fbs= as.factor(data$fbs)
data$restecg= as.factor(data$restecg)
data$exang= as.factor(data$exang)
data$slope= as.factor(data$slope)
data$ca= as.factor(data$ca)
data$thal= as.factor(data$thal)
```

```{r}
str(data)
```


## Imputing Missing/NA
```{r}
sum(is.na(data))
```


## Imputing Outliers - ignore

## scale data /preprocess data - ignore this time

# Exploraring data
## correlation analysis
```{r}

```

## stats summary
```{r}
summary(data)
```

## Linear plot
```{r}

```

# Modeling

## Decision Tree
### Create a list of 80% of the rows in the Original dataset we can use for training
```{r}
index = createDataPartition(data[,1],p=0.80,list = FALSE)
dim(index)
```
### Use 80% of the data to training the model.
```{r}
training = data[index,]
dim(training)
```

```{r}
valid = data[-index,]
dim(valid)
```
### create test harnesses
```{r}
control <- trainControl(method="cv", number=10)
metric <- "Accuracy"
```

### Build a Decision Tree Model using rpart
```{r}
set.seed(7)
fit.rpart <- train(target~., data = training, method="rpart", metric=metric, trControl=control)
```

### Summarize the Results Briefly
```{r}
fit.rpart
```

```{r}
summary(fit.rpart$finalModel)
```
```{r}
suppressMessages(library(rattle))
fancyRpartPlot(fit.rpart$finalModel)
```
### Create Prediciton using Trained Decision Tree'
```{r}
data.pred = predict(fit.rpart, newdata = valid)
table(data.pred, valid$target)
```
### error
```{r}
error.rate = round(mean(data.pred != valid$target,2))
error.rate
```
### Confusion Matrix
```{r}
cm = confusionMatrix(as.factor(data.pred), reference = as.factor(valid$target), mode = "prec_recall")
print(cm)
```

## Random Forest
```{r}
library(AppliedPredictiveModeling)
```

```{r}
transparentTheme(trans = .13)
featurePlot(x = data[, 1:13], 
            y = data$target, 
            plot = "pairs",
            ## Add a key at the top
            auto.key = list(columns = 2))
```

```{r}
set.seed(7)
fit.rf <- train(target~., data = training, method="rf", metric=metric, trControl=control)
```

```{r}
fit.rf
```
```{r}
summary(fit.rf$finalModel)
```

```{r}
plot(fit.rf)
```


```{r}
vi = varImp(fit.rf, scale = FALSE)
plot(vi, top = ncol(training)-1)
```

### create prediciton using Trained random forest
```{r}
data.pred = predict(fit.rf, newdata = valid)
table(data.pred, valid$target)
```

### check error
```{r}
error.rate = round(mean(data.pred != valid$target,2))
error.rate
```

### Confusion Matrix
```{r}
cm = confusionMatrix(as.factor(data.pred), reference = as.factor(valid$target), mode = "prec_recall")
print(cm)
```
## Random Forest Model using extra trees
```{r}
set.seed(7)
fit.extraTrees <- train(target~., data = training, method="rf", metric=metric, trControl=control)
```

```{r}
fit.extraTrees
```

```{r}
summary(fit.extraTrees$finalModel)
```

```{r}
plot(fit.extraTrees)
```

```{r}
data.pred = predict(fit.extraTrees, newdata = valid)
table(data.pred, valid$target)
```

```{r}
error.rate = round(mean(data.pred != valid$target,2))
error.rate
```


```{r}
cm = confusionMatrix(as.factor(data.pred), reference = as.factor(valid$target), mode = "prec_recall")
print(cm)
```


```{r}
results = resamples(list(rf=fit.rf, extraTrees = fit.extraTrees))
summary(results)
```

### visualize comparison
```{r}
dotplot(results)
```


## Logistic Regression
```{r}

```



## K-Means
```{r}
profiling_num(data2)


uns_df <- scale(data2[,10:14])

head(as_tibble(uns_df))


distance <- get_dist(uns_df)
head(distance)


fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))


k2 <- kmeans(uns_df, 
             center = 2,
             nstart = 25  )

str(k2)
k2


fviz_cluster(k2, data = uns_df)
```


```{r}
k3 <- kmeans(uns_df, centers = 3, nstart = 25)
k4 <- kmeans(uns_df, centers = 4, nstart = 25)
k5 <- kmeans(uns_df, centers = 5, nstart = 25)

p1 <- fviz_cluster(k2, geom = "point", data = uns_df)+
  ggtitle("k = 2")
p2 <- fviz_cluster(k3, geom = "point", data = uns_df)+
  ggtitle("k = 3")
p3 <- fviz_cluster(k4, geom = "point", data = uns_df)+
  ggtitle("k = 4")
p4 <- fviz_cluster(k5, geom = "point", data = uns_df)+
  ggtitle("k = 5")

library(gridExtra)
grid.arrange(p1,p2,p3,p2, nrow = 2)
```


###Optimum Cluster Number
```{r}
# Elbow Method
set.seed(123)
fviz_nbclust(uns_df, kmeans, method = "wss")

# Average Silouette Method
fviz_nbclust(uns_df, kmeans, method = "silhouette")


# Gap Statistics
set.seed(123)

gap_stat <- clusGap(uns_df, FUN = kmeans, nstart = 25, K.max = 10, B = 50)

print(gap_stat, method = "firstmax")

fviz_gap_stat


set.seed(123)

final <- kmeans(uns_df, 2, nstart = 25)
final
```


###Descriptive Statistics for Clusters
```{r}
data2[,10:14] %>% 
  mutate(Cluster = final$cluster) %>% 
  group_by(Cluster) %>% 
  summarise_all("mean")
```







